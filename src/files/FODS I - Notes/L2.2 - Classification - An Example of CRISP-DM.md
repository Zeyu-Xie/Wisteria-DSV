# L2.2 - Classification - An Example of CRISP-DM

Our goal is to achieve classification through supervised machine learning.

- A Supervised Machine Learning Method: Try to Predict the Correct Label
- Model trained using traning data, Evaluated on Validation Data, Perform on New Unseen Data

---

## Data Praparation - Train-Validation-Test Split

- Divide a Dataset into Pieces: some untouched for validation and testing
- Partion our Dataset into Training and Testing Subsets -> Ensuring Model Integrity
- Reserve Data for Evaluating Models' Generalization

We will divide the data into three datasets:

1. Train Dataset: Models learn the task from
2. Validation Dataset: Which model is the best?
3. Test Dataset: How good is this model truly?

Notice that we may face the problem of overfitting and underfitting:

Train-Validation-Test Split may depend heavily on which data points end up in the training set and which end up in the test set.

- High Variance: Model too complex and fits the training data too closely
- High Bias: Underfitting the training data leading to poor performance

---

## Modelling - K-Fold Cross-Validation Procedure

- $k$ is the number of subsets
- k-fold cross-validation involves fitting and evaluating $k$ models
- Typical $k$ values: $k=3$, $k=5$, $k=10$, with $10$ represents the most common value

### Steps

1. Divide the data into $k$ subsets (folds), training the model on $k-1$ folds, testing it on the remaining fold. 
1. Repeat the process for $k$ times, average the results.

### Inbalance

One class significantly outnumbers another.

### Advantages

- Mitigates the risk of biased predictions favoring the majority class
- Enhances model performance and fairness (especially: healthcare, finance etc.)

---

## Evaluation - Confusion Matrix

To evaluate the model, we have these methods:

1. Classification
   1. Accuracy
   2. Precision
   3. Recall
   4. F1 Score
2. Clustering
   1. Silhouette Score
   2. Calinski Harabasz Index

This time we will use confusion matrix.

### Confusion Matrix

|                 | Predicted Positive  | Predicted Negative  |
| --------------- | ------------------- | ------------------- |
| Actual Positive | True Positive (TP)  | False Negative (FN) |
| Actual Negative | False Positive (FP) | False Negative (TN) |

The Ratio of Correct Predictions: 

$$
\frac{TP+TN}{TP+TN+FP+FN}
$$